{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"img_cap_module.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"qHVo1kaXVEB6"},"source":["# %matplotlib inline\n","\n","# import os\n","# import numpy as np\n","# import matplotlib.pyplot as plt\n","# import matplotlib.image as mpimg\n","# from keras.models import load_model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rjj9QrPpVEB-"},"source":["# use training token set to create vocabulary\n","train_dir = 'drive/Shareddrives/Bionic_Eye_IoT_Script/Show-And-Tell-Keras/datasets/Flickr8k_text/Flickr_8k.trainImages.txt'\n","token_dir = 'drive/Shareddrives/Bionic_Eye_IoT_Script/Show-And-Tell-Keras/datasets/Flickr8k_text/Flickr8k.token.txt'\n","# the current best trained model\n","model_dir = 'drive/Shareddrives/Bionic_Eye_IoT_Script/Show-And-Tell-Keras/model-params/current_best.h5'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CvnR3G0rdASB","executionInfo":{"status":"ok","timestamp":1638430966264,"user_tz":480,"elapsed":5,"user":{"displayName":"Huake He","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08586107581418236878"}},"outputId":"6bdf252d-6a42-4311-d804-cd53f1c643d9"},"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"Ryo28cnDVXk-"},"source":["# preprocessing/image.py\n","'''\n","Module to preprocess filckr8k image data\n","'''\n","# import numpy as np \n","# import os\n","# from pickle import dump, load\n","\n","# from keras.applications.inception_v3 import InceptionV3\n","# from keras.preprocessing import image\n","# from keras.applications.inception_v3 import preprocess_input\n","# from keras.models import Model\n","\n","# from PIL import Image\n","\n","# base_model = InceptionV3(weights='imagenet')\n","# model_ = Model(inputs=base_model.input, outputs=base_model.get_layer('avg_pool').output)\n","\n","\n","def extract_feature_from_image(file_dir, model):\n","    img = image.load_img(file_dir, target_size=(299, 299))\n","    x = image.img_to_array(img)\n","    x = np.expand_dims(x, axis=0)\n","    x = preprocess_input(x)\n","\n","    # base_model = InceptionV3(weights='imagenet')\n","    # model = Model(inputs=base_model.input, outputs=base_model.get_layer('avg_pool').output)\n","\n","    return model.predict(x)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DRL_44ybVinx"},"source":["# preprocessing/text.py\n","'''\n","Module to preprocess filckr8k text data\n","'''\n","# import os\n","# import string\n","# import numpy as np\n","\n","# from keras.preprocessing.text import Tokenizer\n","# from tensorflow.keras.utils import to_categorical\n","\n","\n","# used in: \n","# create_tokenizer\n","\n","def load_token_text(token_dir):\n","    sents_dict = {}\n","    with open(token_dir, 'r') as f:\n","        for line in f.readlines():\n","            words = line.strip('\\n').split()\n","            img_id = words[0].split('.')[0]\n","            sent = ' '.join(words[1:])\n","\n","            if img_id in sents_dict.keys():\n","                sents_dict[img_id].append(sent)\n","            else:\n","                sents_dict[img_id] = [sent]\n","            \n","    return sents_dict\n","\n","\n","def load_dataset_token(dataset_dir, token_dir, start_end = True):\n","    all_sents = load_token_text(token_dir)\n","\n","    img_ids = []\n","    with open(dataset_dir, 'r') as f:\n","        for line in f.readlines():\n","            img_ids.append(os.path.splitext(line)[0])\n","\n","    sent_list = []\n","    for id in img_ids:\n","        for sent in all_sents[id]:\n","            sent_ = sent\n","            if start_end:\n","                sent_ = 'startseq ' + sent_ + ' endseq'\n","\n","            sent_list.append(sent_)\n","    \n","    return sent_list\n","\n","\n","def create_tokenizer(dataset_dir, token_dir, start_end = True, use_all = False):\n","    # 'num_words = None' for all words in training set\n","    # for example, 'num_words = 6000', means use maximum 6000 words in vocabulary  \n","    num_words = None\n","\n","    sent_list = load_dataset_token(dataset_dir, token_dir, start_end)\n","\n","    if use_all:\n","        tokenizer = Tokenizer()\n","    else:\n","        if num_words:\n","            tokenizer = Tokenizer(num_words)\n","        else:\n","            tokenizer = Tokenizer()\n","\n","    tokenizer.fit_on_texts(sent_list)\n","\n","    return tokenizer\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aoSSDKx4V11K"},"source":["# NIC.py\n","'''\n","File to define the model structure of NIC, based on the paper:\n","\n","https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vinyals_Show_and_Tell_2015_CVPR_paper.pdf\n","\n","model: Define the NIC training model\n","\n","greedy_inference_model: Define the model used in greedy search.\n","                        Please initialize it with trained NIC model by load_weights()\n","\n","image_dense_lstm: Define the encoding part of model used in beam search\n","                  Please initialize it with trained NIC model by load_weights()\n","\n","text_emb_lstm: Define the decoding part of model used in beam search\n","               Please initialize it with trained NIC model by load_weights()\n","'''\n","\n","# import numpy as np\n","# from keras import backend as K\n","# from keras import regularizers\n","# from keras.layers import (LSTM, BatchNormalization, Dense, Dropout, Embedding,\n","#                           Input, Lambda, TimeDistributed)\n","# from keras.models import Model\n","\n","# unit_size = 512\n","\n","def model(vocab_size, max_len, reg):\n","    # Image embedding\n","    inputs1 = Input(shape=(2048,))\n","    X_img = Dropout(0.5)(inputs1)\n","    X_img = Dense(unit_size, use_bias = False, \n","                        kernel_regularizer=regularizers.l2(reg),\n","                        name = 'dense_img')(X_img)\n","    X_img = BatchNormalization(name='batch_normalization_img')(X_img)\n","    X_img = Lambda(lambda x : K.expand_dims(x, axis=1))(X_img)\n","\n","    # Text embedding\n","    inputs2 = Input(shape=(max_len,))\n","    X_text = Embedding(vocab_size, unit_size, mask_zero = True, name = 'emb_text')(inputs2)\n","    X_text = Dropout(0.5)(X_text)\n","\n","    # Initial States\n","    a0 = Input(shape=(unit_size,))\n","    c0 = Input(shape=(unit_size,))\n","\n","    LSTMLayer = LSTM(unit_size, return_sequences = True, return_state = True, dropout=0.5, name = 'lstm')\n","\n","    # Take image embedding as the first input to LSTM\n","    _, a, c = LSTMLayer(X_img, initial_state=[a0, c0])\n","\n","    A, _, _ = LSTMLayer(X_text, initial_state=[a, c])\n","    output = TimeDistributed(Dense(vocab_size, activation='softmax',\n","                                     kernel_regularizer = regularizers.l2(reg), \n","                                     bias_regularizer = regularizers.l2(reg)), name = 'time_distributed_softmax')(A)\n","\n","    return Model(inputs=[inputs1, inputs2, a0, c0], outputs=output, name='NIC')\n","\n","\n","def greedy_inference_model(vocab_size, max_len):\n","    EncoderDense = Dense(unit_size, use_bias=False, name = 'dense_img')\n","    EmbeddingLayer = Embedding(vocab_size, unit_size, mask_zero = True, name = 'emb_text')\n","    LSTMLayer = LSTM(unit_size, return_state = True, name = 'lstm')\n","    SoftmaxLayer = Dense(vocab_size, activation='softmax', name = 'time_distributed_softmax')\n","    BatchNormLayer = BatchNormalization(name='batch_normalization_img')\n","\n","    # Image embedding\n","    inputs1 = Input(shape=(2048,))\n","    X_img = EncoderDense(inputs1)\n","    X_img = BatchNormLayer(X_img)\n","    X_img = Lambda(lambda x : K.expand_dims(x, axis=1))(X_img)\n","\n","    # Text embedding\n","    inputs2 = Input(shape=(1,))\n","    X_text = EmbeddingLayer(inputs2)\n","\n","    # Initial States\n","    a0 = Input(shape=(unit_size,))\n","    c0 = Input(shape=(unit_size,))\n","\n","    a, _, c = LSTMLayer(X_img, initial_state=[a0, c0])\n","\n","    x = X_text\n","\n","    outputs = []\n","    for i in range(max_len):\n","        \n","        a, _, c = LSTMLayer(x, initial_state=[a, c])\n","        output = SoftmaxLayer(a)\n","        outputs.append(output)\n","        x = Lambda(lambda x : K.expand_dims(K.argmax(x)))(output)\n","        x = EmbeddingLayer(x)\n","\n","    return Model(inputs=[inputs1, inputs2, a0, c0], outputs=outputs, name='NIC_greedy_inference_v2')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nFA_LzSuV8Go"},"source":["# evaluate.py\n","'''\n","File to define inference and BLEU evaluation method of NIC, \n","including how to generate captions by given image use greedy or beam search, \n","\n","based on the paper:\n","\n","https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vinyals_Show_and_Tell_2015_CVPR_paper.pdf\n","\n","'''\n","\n","# import math\n","# import os\n","\n","# import numpy as np\n","# from keras.models import load_model\n","# from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n","\n","def decoder(inf_model, tokenizer, features, post_process = True):\n","\n","    '''\n","    Helper funtion of greedy search\n","    '''\n","    assert(features.shape[0]>0 and features.shape[1] == 2048)\n","    \n","    N = features.shape[0]\n","\n","    startseq = np.repeat([tokenizer.word_index['startseq']], N)\n","    a0 = np.zeros([N, unit_size])\n","    c0 = np.zeros([N, unit_size])\n","\n","    #print(\"111\")\n","    # output dims: [32, N, 7378]\n","    y_preds = np.array(inf_model.predict([features, startseq, a0, c0], verbose = 1))\n","    #print(\"222\")\n","    # output dims: [N, 32, 7378]\n","    y_preds = np.transpose(y_preds, axes = [1,0,2])\n","    sequences = np.argmax(y_preds, axis = -1)\n","    sents = tokenizer.sequences_to_texts(sequences)\n","    if post_process:\n","        # post processing: 'endseq'\n","        sents_pp = []\n","        for sent in sents:\n","            if 'endseq' in sent.split():\n","                words = sent.split()\n","                sents_pp.append(' '.join(words[:words.index('endseq')]))\n","            else:\n","                sents_pp.append(sent)\n","        sents = sents_pp\n","    return sents"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"juh6q4H5VEB_"},"source":["## Greedy inference"]},{"cell_type":"code","metadata":{"id":"V2I1V9-6i6Mr"},"source":["# load vocabulary\n","tokenizer = create_tokenizer(train_dir, token_dir, start_end = True, use_all=True)\n","\n","# set relevent parameters\n","vocab_size  = tokenizer.num_words or (len(tokenizer.word_index)+1)\n","max_len = 24 # use 24 as maximum sentence's length when training the model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XGqcBwjpVECB"},"source":["NIC_inference = greedy_inference_model(vocab_size, max_len)\n","NIC_inference.load_weights(model_dir, by_name = True, skip_mismatch=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8qtdUYM1VECB"},"source":["def generate_caption_from_file(file_dir,model):\n","    # Encoder\n","    img_feature = extract_feature_from_image(file_dir,model)\n","    # Decoder\n","    caption = decoder(NIC_inference, tokenizer, img_feature, True)\n","    \n","    return caption"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-HbPD7MaunYd"},"source":["The run API is required by the interface for all modules. This is the method  the server runs to start this module."]},{"cell_type":"code","metadata":{"id":"-NQKRSuxVECC"},"source":["def run(num_imgs,model):\n","\n","  for idx in range (1, num_imgs + 1):\n","    image_file_dir = '/content/drive/Shareddrives/Bionic_Eye_IoT_Data/Input/' + str(idx) + '_input.jpg'\n","    # image_file_dir = 'drive/Shareddrives/Bionic_Eye_IoT_Script/Show-And-Tell-Keras/put-your-image-here/example.jpg'\n","\n","    # display image\n","    img = mpimg.imread(image_file_dir)\n","    #plt.imshow(img)\n","\n","    #generate caption\n","    caption = generate_caption_from_file(image_file_dir,model)\n","    #plt.show()\n","    # Write strings to a file\n","    out_name = '/content/drive/Shareddrives/Bionic_Eye_IoT_Data/Output/' + str(idx) + '_output.txt'\n","    out_txt = open(out_name,'w')\n","    out_txt.write(caption[0])\n","    out_txt.close()\n","\n","    #print(caption)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oJLsZR7A6LqJ"},"source":["Driver code for this module."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GJA9xhkB6MCF","executionInfo":{"status":"ok","timestamp":1638431027377,"user_tz":480,"elapsed":33943,"user":{"displayName":"Huake He","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08586107581418236878"}},"outputId":"6a5c541d-baed-4d2f-b060-10ef83f54ee1"},"source":["num_imgs = len(os.listdir('/content/drive/Shareddrives/Bionic_Eye_IoT_Data/Input'))\n","run(num_imgs,model_)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1\n","1/1 [==============================] - 32s 32s/step\n","2\n","['a girl in a bikini is jumping into the water']\n"]}]}]}