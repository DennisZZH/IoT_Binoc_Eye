{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"img_cap_module.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"s8-uNuticHkO"},"source":["# use training token set to create vocabulary\n","train_dir = 'drive/Shareddrives/Bionic_Eye_IoT_Script/Show-And-Tell-Keras/datasets/Flickr8k_text/Flickr_8k.trainImages.txt'\n","token_dir = 'drive/Shareddrives/Bionic_Eye_IoT_Script/Show-And-Tell-Keras/datasets/Flickr8k_text/Flickr8k.token.txt'\n","# the current best trained model\n","model_dir = 'drive/Shareddrives/Bionic_Eye_IoT_Script/Show-And-Tell-Keras/model-params/current_best.h5'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0SrODLWQcLyS","executionInfo":{"status":"ok","timestamp":1638856384159,"user_tz":480,"elapsed":562,"user":{"displayName":"Zihao Zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15865091318528054331"}}},"source":["def extract_feature_from_image(file_dir, model):\n","    img = image.load_img(file_dir, target_size=(299, 299))\n","    x = image.img_to_array(img)\n","    x = np.expand_dims(x, axis=0)\n","    x = preprocess_input(x)\n","\n","    # base_model = InceptionV3(weights='imagenet')\n","    # model = Model(inputs=base_model.input, outputs=base_model.get_layer('avg_pool').output)\n","\n","    return model.predict(x)\n","\n","def load_token_text(token_dir):\n","    sents_dict = {}\n","    with open(token_dir, 'r') as f:\n","        for line in f.readlines():\n","            words = line.strip('\\n').split()\n","            img_id = words[0].split('.')[0]\n","            sent = ' '.join(words[1:])\n","\n","            if img_id in sents_dict.keys():\n","                sents_dict[img_id].append(sent)\n","            else:\n","                sents_dict[img_id] = [sent]\n","            \n","    return sents_dict\n","\n","\n","def load_dataset_token(dataset_dir, token_dir, start_end = True):\n","    all_sents = load_token_text(token_dir)\n","\n","    img_ids = []\n","    with open(dataset_dir, 'r') as f:\n","        for line in f.readlines():\n","            img_ids.append(os.path.splitext(line)[0])\n","\n","    sent_list = []\n","    for id in img_ids:\n","        for sent in all_sents[id]:\n","            sent_ = sent\n","            if start_end:\n","                sent_ = 'startseq ' + sent_ + ' endseq'\n","\n","            sent_list.append(sent_)\n","    \n","    return sent_list\n","\n","\n","def create_tokenizer(dataset_dir, token_dir, start_end = True, use_all = False):\n","    # 'num_words = None' for all words in training set\n","    # for example, 'num_words = 6000', means use maximum 6000 words in vocabulary  \n","    num_words = None\n","\n","    sent_list = load_dataset_token(dataset_dir, token_dir, start_end)\n","\n","    if use_all:\n","        tokenizer = Tokenizer()\n","    else:\n","        if num_words:\n","            tokenizer = Tokenizer(num_words)\n","        else:\n","            tokenizer = Tokenizer()\n","\n","    tokenizer.fit_on_texts(sent_list)\n","\n","    return tokenizer\n","\n","def model(vocab_size, max_len, reg):\n","    # Image embedding\n","    inputs1 = Input(shape=(2048,))\n","    X_img = Dropout(0.5)(inputs1)\n","    X_img = Dense(unit_size, use_bias = False, \n","                        kernel_regularizer=regularizers.l2(reg),\n","                        name = 'dense_img')(X_img)\n","    X_img = BatchNormalization(name='batch_normalization_img')(X_img)\n","    X_img = Lambda(lambda x : K.expand_dims(x, axis=1))(X_img)\n","\n","    # Text embedding\n","    inputs2 = Input(shape=(max_len,))\n","    X_text = Embedding(vocab_size, unit_size, mask_zero = True, name = 'emb_text')(inputs2)\n","    X_text = Dropout(0.5)(X_text)\n","\n","    # Initial States\n","    a0 = Input(shape=(unit_size,))\n","    c0 = Input(shape=(unit_size,))\n","\n","    LSTMLayer = LSTM(unit_size, return_sequences = True, return_state = True, dropout=0.5, name = 'lstm')\n","\n","    # Take image embedding as the first input to LSTM\n","    _, a, c = LSTMLayer(X_img, initial_state=[a0, c0])\n","\n","    A, _, _ = LSTMLayer(X_text, initial_state=[a, c])\n","    output = TimeDistributed(Dense(vocab_size, activation='softmax',\n","                                     kernel_regularizer = regularizers.l2(reg), \n","                                     bias_regularizer = regularizers.l2(reg)), name = 'time_distributed_softmax')(A)\n","\n","    return Model(inputs=[inputs1, inputs2, a0, c0], outputs=output, name='NIC')\n","\n","\n","def greedy_inference_model(vocab_size, max_len):\n","    EncoderDense = Dense(unit_size, use_bias=False, name = 'dense_img')\n","    EmbeddingLayer = Embedding(vocab_size, unit_size, mask_zero = True, name = 'emb_text')\n","    LSTMLayer = LSTM(unit_size, return_state = True, name = 'lstm')\n","    SoftmaxLayer = Dense(vocab_size, activation='softmax', name = 'time_distributed_softmax')\n","    BatchNormLayer = BatchNormalization(name='batch_normalization_img')\n","\n","    # Image embedding\n","    inputs1 = Input(shape=(2048,))\n","    X_img = EncoderDense(inputs1)\n","    X_img = BatchNormLayer(X_img)\n","    X_img = Lambda(lambda x : K.expand_dims(x, axis=1))(X_img)\n","\n","    # Text embedding\n","    inputs2 = Input(shape=(1,))\n","    X_text = EmbeddingLayer(inputs2)\n","\n","    # Initial States\n","    a0 = Input(shape=(unit_size,))\n","    c0 = Input(shape=(unit_size,))\n","\n","    a, _, c = LSTMLayer(X_img, initial_state=[a0, c0])\n","\n","    x = X_text\n","\n","    outputs = []\n","    for i in range(max_len):\n","        \n","        a, _, c = LSTMLayer(x, initial_state=[a, c])\n","        output = SoftmaxLayer(a)\n","        outputs.append(output)\n","        x = Lambda(lambda x : K.expand_dims(K.argmax(x)))(output)\n","        x = EmbeddingLayer(x)\n","\n","    return Model(inputs=[inputs1, inputs2, a0, c0], outputs=outputs, name='NIC_greedy_inference_v2')\n","\n","def decoder(inf_model, tokenizer, features, post_process = True):\n","\n","    '''\n","    Helper funtion of greedy search\n","    '''\n","    assert(features.shape[0]>0 and features.shape[1] == 2048)\n","    \n","    N = features.shape[0]\n","\n","    startseq = np.repeat([tokenizer.word_index['startseq']], N)\n","    a0 = np.zeros([N, unit_size])\n","    c0 = np.zeros([N, unit_size])\n","\n","    #print(\"111\")\n","    # output dims: [32, N, 7378]\n","    y_preds = np.array(inf_model.predict([features, startseq, a0, c0], verbose = 1))\n","    #print(\"222\")\n","    # output dims: [N, 32, 7378]\n","    y_preds = np.transpose(y_preds, axes = [1,0,2])\n","    sequences = np.argmax(y_preds, axis = -1)\n","    sents = tokenizer.sequences_to_texts(sequences)\n","    if post_process:\n","        # post processing: 'endseq'\n","        sents_pp = []\n","        for sent in sents:\n","            if 'endseq' in sent.split():\n","                words = sent.split()\n","                sents_pp.append(' '.join(words[:words.index('endseq')]))\n","            else:\n","                sents_pp.append(sent)\n","        sents = sents_pp\n","    return sents"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ch0T7BLicbxy"},"source":["Greedy Inference"]},{"cell_type":"code","metadata":{"id":"am88wSPPcdHd","executionInfo":{"status":"error","timestamp":1638856390373,"user_tz":480,"elapsed":181,"user":{"displayName":"Zihao Zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15865091318528054331"}},"outputId":"190bcc0c-57e1-4f9f-e08b-a173463c6de3","colab":{"base_uri":"https://localhost:8080/","height":236}},"source":["# load vocabulary\n","tokenizer = create_tokenizer(train_dir, token_dir, start_end = True, use_all=True)\n","\n","# set relevent parameters\n","vocab_size  = tokenizer.num_words or (len(tokenizer.word_index)+1)\n","max_len = 24 # use 24 as maximum sentence's length when training the model\n","NIC_inference = greedy_inference_model(vocab_size, max_len)\n","NIC_inference.load_weights(model_dir, by_name = True, skip_mismatch=True)\n","def generate_caption_from_file(file_dir,model):\n","    # Encoder\n","    img_feature = extract_feature_from_image(file_dir,model)\n","    # Decoder\n","    caption = decoder(NIC_inference, tokenizer, img_feature, True)\n","    \n","    return caption\n","\n"],"execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-16718eb5236b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# set relevent parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvocab_size\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_words\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_dir' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"Q1InVHg9cjWl"},"source":["The run API is required by the interface for all modules. This is the method  the server runs to start this module."]},{"cell_type":"code","metadata":{"id":"0zy2-QxAcj6Y"},"source":["def run(num_imgs,model):\n","\n","  for idx in range (1, num_imgs + 1):\n","    image_file_dir = '/content/drive/Shareddrives/Bionic_Eye_IoT_Data/Input/' + str(idx) + '_input.jpg'\n","    # image_file_dir = 'drive/Shareddrives/Bionic_Eye_IoT_Script/Show-And-Tell-Keras/put-your-image-here/example.jpg'\n","\n","    # display image\n","    img = mpimg.imread(image_file_dir)\n","    #plt.imshow(img)\n","\n","    #generate caption\n","    caption = generate_caption_from_file(image_file_dir,model)\n","    #plt.show()\n","    # Write strings to a file\n","    out_name = '/content/drive/Shareddrives/Bionic_Eye_IoT_Data/Output/' + str(idx) + '_output.txt'\n","    out_txt = open(out_name,'w')\n","    out_txt.write(caption[0])\n","    out_txt.close()\n","\n","    #print(caption)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y3IGQrczcl_F"},"source":["Driver code for this module."]},{"cell_type":"code","metadata":{"id":"Nmzy22amcmUs"},"source":["num_imgs = len(os.listdir('/content/drive/Shareddrives/Bionic_Eye_IoT_Data/Input')) - 1\n","run(num_imgs,model_)"],"execution_count":null,"outputs":[]}]}